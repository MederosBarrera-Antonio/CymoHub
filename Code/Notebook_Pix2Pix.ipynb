{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b810654f",
   "metadata": {},
   "source": [
    "\n",
    "# Notebook for U-Net and Attention U-Net models\n",
    "\n",
    "___\n",
    "\n",
    "## PAPER: Comparison of Conventional Machine Learning and Convolutional Deep Learning models for Seagrass Mapping using Satellite Imagery\n",
    "\n",
    "#### Antonio Mederos-Barrera (mederosbarrera.antonio@gmail.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab14f295",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec36133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import scipy.io\n",
    "from scipy.stats import mode\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "# Utils\n",
    "sys.path.append(os.path.abspath('.'))\n",
    "from utils.class_weights import obtain_class_weights\n",
    "from utils.dice_loss_function import dice_loss\n",
    "from utils.early_stopping import EarlyStopping\n",
    "from utils.error_metrics import error_metrics\n",
    "from utils.load_data import find_files, mean_std_dataset, data_train_test, SegmentationDataset\n",
    "from utils.map_estimation_functions import mode_no_zeros, numpy_to_torch, obtain_mode_no_zeros, predict_model, sliding_window\n",
    "from utils.model_initialization import initialize_weights_kaiming\n",
    "from models.unet import UNet, DoubleConv, Down, Up, OutConv\n",
    "from models.patchgan import PatchGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8605e3",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514a92db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU use\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "PIN_MEMORY = True if DEVICE == \"cuda\" else False\n",
    "\n",
    "# Reproducibility\n",
    "def set_manual_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        \n",
    "SEED = 0                                                  # Random seed for reproducibility\n",
    "set_manual_seed(SEED)\n",
    "\n",
    "BATCH_SIZE = \"all\"                                        # Batch size (can be \"all\" to use all the training and test dataset)\n",
    "\n",
    "NUM_EPOCHS = 2000                                         # Epochs (upper limit in case Early stopping does not converge)\n",
    "\n",
    "PATIENCE_EARLYSTOPPING = 140                              # Patiente of Early Stopping\n",
    "\n",
    "INPUT_WIDTH = 70                                          # Image size (first dimension)\n",
    "INPUT_HEIGHT = 70                                         # Image size (second dimension)\n",
    "NUM_CHANNELS = 8                                          # Image channels (third dimension)\n",
    "\n",
    "# Path to the dataset\n",
    "IMAGE_DATASET_PATH = \"C:/Path/To/Data\"\n",
    "NUM_CLASSES = 5                                           # Number of classes\n",
    "VECTOR_IDX_TEST = [6,10,9,7,13,17,19,25,29,34,37,42,46,50]# Vector images for the test dataset\n",
    "\n",
    "PATH = \"C:/Path/To/Save/Results/\"\n",
    "MODEL_PATH = PATH + \"unet_\"\n",
    "PLOT_PATH = PATH + \"plot_\"\n",
    "NPY_PATH = PATH + \"lossfunc_\"\n",
    "\n",
    "# Models hyperparameters (LR, L2, ExpSchedGamma, AttGates, LambdaL1)\n",
    "Model1 = [0.001, 0.0001, None, False, 100]\n",
    "\n",
    "Models = [Model1]\n",
    "\n",
    "# Estimation\n",
    "IMG_PATH = 'C:Path/To/Cmomplete/WorldView/Image.mat' # Image\n",
    "mymap = np.array([  # Colors\n",
    "    [0, 0, 0],      # Land\n",
    "    [139, 0, 0],    # Red algae\n",
    "    [0, 139, 1],    # C. nodosa (sebadal)\n",
    "    [142, 87, 2],   # Rock\n",
    "    [255, 255, 0]   # Sand\n",
    "])\n",
    "window_size = (70, 70) # Slide windows\n",
    "stride = 5          # Slide windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6c81e6",
   "metadata": {},
   "source": [
    " ### Training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b56ed40",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def split_params_for_weight_decay(model):\n",
    "    decay, no_decay = [], []\n",
    "    for n, p in model.named_parameters():\n",
    "        if not p.requires_grad:\n",
    "            continue\n",
    "        is_norm = any(k in n.lower() for k in [\"bn\", \"norm\", \"bias\"])\n",
    "        is_att  = \"att\" in n.lower()\n",
    "        if is_norm or is_att:\n",
    "            no_decay.append(p)\n",
    "        else:\n",
    "            decay.append(p)\n",
    "    return decay, no_decay\n",
    "\n",
    "def plot_error_curve(matrix_data_train_test, limit_G=None, limit_L1=None):\n",
    "    md = matrix_data_train_test.copy()\n",
    "    aux_G = md[:, 0]\n",
    "    aux_L1 = md[:, 1]\n",
    "    if limit_G is not None:\n",
    "        aux_G[aux_G > limit_G] = limit_G\n",
    "    if limit_L1 is not None:\n",
    "        aux_L1[aux_L1 > limit_L1] = limit_L1\n",
    "    plt.style.use(\"ggplot\")\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    axes[0].plot(aux_G, label=\"Training loss (G_total)\", color=\"tab:blue\")\n",
    "    axes[0].set_xlabel(\"Epoch #\")\n",
    "    axes[0].set_ylabel(\"Loss\")\n",
    "    axes[0].set_title(\"Training Loss (G_total)\")\n",
    "    axes[0].legend(loc=\"upper right\")\n",
    "    axes[1].plot(aux_L1, label=\"Val L1\", color=\"tab:orange\")\n",
    "    axes[1].set_xlabel(\"Epoch #\")\n",
    "    axes[1].set_ylabel(\"Loss\")\n",
    "    axes[1].set_title(\"Validation L1 Loss\")\n",
    "    axes[1].legend(loc=\"upper right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def delete_file(directory, start, end):\n",
    "    for file in os.listdir(directory):\n",
    "        if file.startswith(start) and file.endswith(end):\n",
    "            file_path = os.path.join(directory, file)\n",
    "            if os.path.isfile(file_path):\n",
    "                try:\n",
    "                    os.remove(file_path)\n",
    "                except OSError as e:\n",
    "                    print(f\"Error deleting the file {file_path}: {e}\")\n",
    "\n",
    "trainLoader, testLoader, trainDS, testDS, BATCH_SIZE_TRAIN, BATCH_SIZE_TEST = data_train_test(IMAGE_DATASET_PATH,\n",
    "                                                                                              INPUT_WIDTH,\n",
    "                                                                                              INPUT_HEIGHT,\n",
    "                                                                                              NUM_CLASSES,\n",
    "                                                                                              BATCH_SIZE,\n",
    "                                                                                              PIN_MEMORY,\n",
    "                                                                                              VECTOR_IDX_TEST)\n",
    "class_weights = obtain_class_weights(trainLoader, NUM_CLASSES, DEVICE)\n",
    "\n",
    "\n",
    "indx_print = 0\n",
    "\n",
    "criterion_GAN = nn.BCEWithLogitsLoss()\n",
    "criterion_L1 = nn.L1Loss()\n",
    "\n",
    "for mdl_hyp in Models:\n",
    "    \n",
    "    lr_value = mdl_hyp[0]\n",
    "    reg_L2_value = mdl_hyp[1]\n",
    "    scheduler_gamma = mdl_hyp[2]\n",
    "    use_attention_gates = mdl_hyp[3]\n",
    "    lambda_L1 = mdl_hyp[4]\n",
    "    \n",
    "    # Initial print\n",
    "    indx_print += 1\n",
    "    print(\"[\",indx_print,\"of\",len(Models),\"]\")\n",
    "\n",
    "    # Random seed\n",
    "    set_manual_seed(SEED)\n",
    "\n",
    "    # Model\n",
    "    G = UNet(n_channels=NUM_CHANNELS, n_classes=NUM_CLASSES, use_attention_gates=use_attention_gates)\n",
    "    D = PatchGAN(in_channels=NUM_CHANNELS + NUM_CLASSES)\n",
    "    \n",
    "    initialize_weights_kaiming(G)\n",
    "    G.to(DEVICE)\n",
    "    initialize_weights_kaiming(D)\n",
    "    D.to(DEVICE)\n",
    "\n",
    "    # Optimizer\n",
    "    if reg_L2_value is None:\n",
    "        opt_G = Adam(G.parameters(), lr=lr_value)\n",
    "        opt_D = Adam(D.parameters(), lr=lr_value/2)\n",
    "    else:\n",
    "        opt_G = Adam(G.parameters(), lr=lr_value,   weight_decay=reg_L2_value)\n",
    "        opt_D = Adam(D.parameters(), lr=lr_value/2)\n",
    "\n",
    "    # Scheduler LR\n",
    "    if scheduler_gamma is not None:\n",
    "        scheduler_G = ExponentialLR(opt_G, gamma=scheduler_gamma)\n",
    "        scheduler_D = ExponentialLR(opt_D, gamma=scheduler_gamma)\n",
    "        lrs_values_G, lrs_values_D = [], []\n",
    "\n",
    "    # Data\n",
    "    trainLoader, testLoader, trainDS, testDS, BATCH_SIZE_TRAIN, BATCH_SIZE_TEST = data_train_test(IMAGE_DATASET_PATH,\n",
    "                                                                                                  INPUT_WIDTH,\n",
    "                                                                                                  INPUT_HEIGHT,\n",
    "                                                                                                  NUM_CLASSES,\n",
    "                                                                                                  BATCH_SIZE,\n",
    "                                                                                                  PIN_MEMORY,\n",
    "                                                                                                  VECTOR_IDX_TEST)\n",
    "\n",
    "    # Steps\n",
    "    trainSteps = len(trainDS) // BATCH_SIZE_TRAIN\n",
    "    testSteps  = len(testDS)  // BATCH_SIZE_TEST\n",
    "\n",
    "    # Early Stopping\n",
    "    early_stopping = EarlyStopping(patience=PATIENCE_EARLYSTOPPING,\n",
    "                                   path_base=MODEL_PATH,\n",
    "                                   mode='min',\n",
    "                                   loss_func=\"pix2pix\",\n",
    "                                   lr_value=lr_value,\n",
    "                                   scheduler_gamma=scheduler_gamma,\n",
    "                                   verbose=False,\n",
    "                                   min_delta=0.00001,\n",
    "                                   reg_L2_value=reg_L2_value,\n",
    "                                   use_attention_gates=use_attention_gates)\n",
    "\n",
    "    # Training and test epochs\n",
    "    matrix_data_train_test = np.zeros((NUM_EPOCHS, 2))\n",
    "    for e in tqdm(range(NUM_EPOCHS)):\n",
    "\n",
    "        # Train\n",
    "        G.train()\n",
    "        D.train()\n",
    "        \n",
    "        total_G_loss = 0.0\n",
    "        total_val_L1 = 0.0\n",
    "        for (i, (x, y)) in enumerate(trainLoader):\n",
    "            (x, y) = (x.to(DEVICE), y.to(DEVICE))\n",
    "\n",
    "            # Discriminator Training\n",
    "            opt_D.zero_grad()\n",
    "            with torch.no_grad():\n",
    "                fake_y = G(x)\n",
    "            pred_real = D(x, y)\n",
    "            pred_fake = D(x, fake_y.detach())\n",
    "            loss_D_real = criterion_GAN(pred_real, torch.ones_like(pred_real))\n",
    "            loss_D_fake = criterion_GAN(pred_fake, torch.zeros_like(pred_fake))\n",
    "            loss_D = 0.5 * (loss_D_real + loss_D_fake)\n",
    "            loss_D.backward()\n",
    "            opt_D.step()\n",
    "\n",
    "            # Generator Training\n",
    "            opt_G.zero_grad()\n",
    "            fake_y = G(x)\n",
    "            pred_fake_for_G = D(x, fake_y)\n",
    "            loss_G_GAN = criterion_GAN(pred_fake_for_G, torch.ones_like(pred_fake_for_G))\n",
    "            loss_G_L1  = criterion_L1(fake_y, y) * lambda_L1\n",
    "            loss_G = loss_G_GAN + loss_G_L1\n",
    "            loss_G.backward()\n",
    "            opt_G.step()\n",
    "\n",
    "            total_G_loss += loss_G.detach().item()\n",
    "\n",
    "        # Test\n",
    "        with torch.no_grad():\n",
    "            G.eval()\n",
    "            D.eval()\n",
    "            for (i, (x, y)) in enumerate(testLoader):\n",
    "                (x, y) = (x.to(DEVICE), y.to(DEVICE))\n",
    "                fake_y = G(x)\n",
    "                val_L1 = criterion_L1(fake_y, y)\n",
    "                total_val_L1 += val_L1.item()\n",
    "        \n",
    "        # Training and test losses\n",
    "        avg_G_loss = total_G_loss / trainSteps\n",
    "        avg_val_L1 = total_val_L1 / testSteps\n",
    "        matrix_data_train_test[e, 0] = avg_G_loss\n",
    "        matrix_data_train_test[e, 1] = avg_val_L1\n",
    "\n",
    "        # Actualization of Scheduler LR\n",
    "        if scheduler_gamma is not None:\n",
    "            lrs_values_G.append(scheduler_G.get_last_lr()[0])\n",
    "            lrs_values_D.append(scheduler_D.get_last_lr()[0])\n",
    "            scheduler_G.step()\n",
    "            scheduler_D.step()\n",
    "\n",
    "        # Early Stopping\n",
    "        early_stopping.epoch = e\n",
    "        early_stopping.lr = lr_value\n",
    "        early_stopping(avg_val_L1, G)\n",
    "        if early_stopping.early_stop:\n",
    "\n",
    "            best_epoch = int(e-PATIENCE_EARLYSTOPPING)\n",
    "            print(\"Early stopping in the epoch:\", best_epoch)\n",
    "\n",
    "            # Save\n",
    "            NPY_PATH_ = NPY_PATH+'pix2pix_LR-'+str(lr_value)+'_ExpSchedGamma-'+str(scheduler_gamma)+'_L2-'+str(reg_L2_value)+'_EPOCHS-'+str(best_epoch)+\"_EarlyStopping_AG-\"+str(use_attention_gates)+\".npy\"\n",
    "            np.save(NPY_PATH_, matrix_data_train_test[:best_epoch+1,:])\n",
    "            NPY_PATH_ALL_ = NPY_PATH+'pix2pix_LR-'+str(lr_value)+'_ExpSchedGamma-'+str(scheduler_gamma)+'_L2-'+str(reg_L2_value)+'_EPOCHS-'+str(best_epoch)+\"_EarlyStopping_AG-\"+str(use_attention_gates)+\"_ALL.npy\"\n",
    "            np.save(NPY_PATH_ALL_, matrix_data_train_test)\n",
    "\n",
    "            # Error Curves\n",
    "            MODEL_PATH_ = MODEL_PATH+'pix2pix_LR-'+str(lr_value)+'_ExpSchedGamma-'+str(scheduler_gamma)+'_L2-'+str(reg_L2_value)+'_EPOCHS-'+str(best_epoch)+\"_EarlyStopping_AG-\"+str(use_attention_gates)+\".pth\"\n",
    "            print(MODEL_PATH_)\n",
    "            plot_error_curve(matrix_data_train_test[:best_epoch+1,:])\n",
    "            plot_error_curve(matrix_data_train_test[:best_epoch+1,:], limit_G=50.0, limit_L1=1.0)\n",
    "\n",
    "            if scheduler_gamma is not None:\n",
    "                # LRs values (Schedulers)\n",
    "                lrs_values_G_np = np.array(lrs_values_G)\n",
    "                lrs_values_D_np = np.array(lrs_values_D)\n",
    "                plt.figure()\n",
    "                plt.style.use(\"ggplot\")\n",
    "                plt.plot(lrs_values_G_np[:best_epoch+1], label=\"LR_G\")\n",
    "                plt.plot(lrs_values_D_np[:best_epoch+1], label=\"LR_D\")\n",
    "                plt.xlabel('Epochs #')\n",
    "                plt.ylabel('Learning Rate')\n",
    "                plt.title(\"LR Exponential Scheduler. Gamma: \"+str(scheduler_gamma))\n",
    "                plt.grid(True)\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "\n",
    "            break\n",
    "\n",
    "    # Early stopping is not triggered\n",
    "    if not early_stopping.early_stop:\n",
    "        print(\"Early stopping is not triggered\")\n",
    "        plot_error_curve(matrix_data_train_test)\n",
    "        plot_error_curve(matrix_data_train_test, limit_G=50.0, limit_L1=1.0)\n",
    "\n",
    "        if scheduler_gamma is not None:\n",
    "            plt.figure()\n",
    "            plt.style.use(\"ggplot\")\n",
    "            plt.plot(lrs_values_G, label=\"LR_G\")\n",
    "            plt.plot(lrs_values_D, label=\"LR_D\")\n",
    "            plt.xlabel('Epochs #')\n",
    "            plt.ylabel('Learning Rate')\n",
    "            plt.title(\"LR Exponential Scheduler. Gamma: \"+str(scheduler_gamma))\n",
    "            plt.grid(True)\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "        MODEL_PATH_G_LAST_ = MODEL_PATH+'pix2pix_LR-'+str(lr_value)+'_ExpSchedGamma-'+str(scheduler_gamma)+'_L2-'+str(reg_L2_value)+'_EPOCHS-'+str(best_epoch)+\"_EarlyStopping_AG-\"+str(use_attention_gates)+\".pth\"\n",
    "        torch.save(G.state_dict(), MODEL_PATH_G_LAST_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0368c8",
   "metadata": {},
   "source": [
    "### Error metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbf3601",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FINAL_MODELS = [PATH+archivo for archivo in os.listdir(PATH) if archivo.startswith(MODEL_PATH[len(PATH):]) and archivo.endswith(\".pth\")]\n",
    "\n",
    "CSV_PATH = [model.replace('unet_', 'csv_') for model in FINAL_MODELS]\n",
    "CSV_PATH = [model.replace('.pth', '.csv') for model in CSV_PATH]\n",
    "\n",
    "for indx_model, model in enumerate(FINAL_MODELS):\n",
    "\n",
    "    print(\"[\",indx_model+1,\"of\",len(FINAL_MODELS),\"]\")\n",
    "    \n",
    "    # Model\n",
    "    mdl = torch.load(model, weights_only=False)\n",
    "    mdl.to(DEVICE)\n",
    "    print(model)\n",
    "          \n",
    "    # Metrics initialization\n",
    "    accuracy = np.zeros((NUM_CLASSES,))\n",
    "    precision = np.zeros((NUM_CLASSES,))\n",
    "    recall = np.zeros((NUM_CLASSES,))\n",
    "    f1 = np.zeros((NUM_CLASSES,))\n",
    "    iou = np.zeros((NUM_CLASSES,))\n",
    "    dice = np.zeros((NUM_CLASSES,))\n",
    "                \n",
    "    # Test error metrics\n",
    "    with torch.no_grad():\n",
    "        mdl.eval()\n",
    "        testSteps = 0\n",
    "        for (i, (x, y)) in enumerate(testLoader):\n",
    "            (x, y) = (x.to(DEVICE), y.to(DEVICE))\n",
    "            pred = mdl(x)\n",
    "            pred = torch.argmax(pred, dim=1)\n",
    "            pred = F.one_hot(pred, num_classes=x.shape[1])\n",
    "            pred = pred.permute(0, 3, 1, 2)\n",
    "            y = y.cpu().detach().numpy()\n",
    "            pred = pred.cpu().detach().numpy()\n",
    "                                \n",
    "            accuracy_, precision_, recall_, f1_, iou_, dice_ = error_metrics(y, pred)\n",
    "            accuracy += accuracy_\n",
    "            precision += precision_\n",
    "            recall += recall_\n",
    "            f1 += f1_\n",
    "            iou += iou_\n",
    "            dice += dice_\n",
    "            \n",
    "            testSteps +=1\n",
    "                    \n",
    "        accuracy /= testSteps\n",
    "        precision /= testSteps\n",
    "        recall /= testSteps\n",
    "        f1 /= testSteps\n",
    "        iou /= testSteps\n",
    "        dice /= testSteps\n",
    "        \n",
    "        # Values and mean without land class (not considered in ML models)\n",
    "        accuracy =  np.concatenate((accuracy, [-1 if -1 in accuracy else np.mean(accuracy[1:])]))\n",
    "        precision =  np.concatenate((precision, [-1 if -1 in precision else np.mean(precision[1:])]))\n",
    "        recall =  np.concatenate((recall, [-1 if -1 in recall else np.mean(recall[1:])]))\n",
    "        f1 =  np.concatenate((f1, [-1 if -1 in f1 else np.mean(f1[1:])]))\n",
    "        iou =  np.concatenate((iou, [-1 if -1 in iou else np.mean(iou[1:])]))\n",
    "        dice =  np.concatenate((dice, [-1 if -1 in dice else np.mean(dice[1:])]))\n",
    "        print(\"Metrics (Acc, Prec, Recall, F1, IoU, Dice):\\n\", \n",
    "                accuracy,'\\n',\n",
    "                precision,'\\n',\n",
    "                recall,'\\n',\n",
    "                f1,'\\n',\n",
    "                iou,'\\n',\n",
    "                dice,'\\n')\n",
    "\n",
    "        # Save metrics in CSV file\n",
    "        metrics_dict = {\n",
    "            'Accuracy': accuracy,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1-score': f1,\n",
    "            'IoU': iou,\n",
    "            'Dice Coefficient': dice\n",
    "        }\n",
    "        columns = ['Land', 'Red algae', 'C. nodosa', 'Rock', 'Sand', 'Mean (without land)']\n",
    "        df_metrics = pd.DataFrame(metrics_dict, index=columns).transpose()\n",
    "        df_metrics.to_csv(CSV_PATH[indx_model], sep=';', index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567b25a0",
   "metadata": {},
   "source": [
    "### Map estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b5291a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FINAL_MODELS = [PATH+archivo for archivo in os.listdir(PATH) if archivo.startswith(MODEL_PATH[len(PATH):]) and archivo.endswith(\".pth\")]\n",
    "\n",
    "SAVE_PATH = [model.replace('unet_', 'plot_') for model in FINAL_MODELS]\n",
    "SAVE_PATH = [model.replace('.pth', '.mat') for model in SAVE_PATH]\n",
    "\n",
    "mat_data_image = scipy.io.loadmat(IMG_PATH)\n",
    "filtered_keys_image = [key for key in mat_data_image.keys() if \"__\" not in key]\n",
    "image = mat_data_image[filtered_keys_image[0]]\n",
    "\n",
    "mean, std = mean_std_dataset(IMAGE_DATASET_PATH, INPUT_WIDTH, INPUT_HEIGHT, NUM_CLASSES, BATCH_SIZE, PIN_MEMORY)\n",
    "image = numpy_to_torch(image, mean, std)\n",
    "\n",
    "# Estimation\n",
    "for indx_model, path_model in tqdm(enumerate(FINAL_MODELS)):\n",
    "    \n",
    "    print(\"[\",indx_model+1,\"of\",len(FINAL_MODELS),\"]\")\n",
    "    \n",
    "    # Model\n",
    "    mdl = torch.load(path_model, weights_only=False)\n",
    "    mdl.to(DEVICE)\n",
    "\n",
    "    # Slide window\n",
    "    pred_image = sliding_window(image, window_size, stride, mdl, DEVICE)\n",
    "\n",
    "    # Mode without zeros values\n",
    "    pred_image_mode = obtain_mode_no_zeros(pred_image)\n",
    "    \n",
    "    # Presentation\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plt.title(SAVE_PATH[indx_model])\n",
    "    plt.imshow(mymap[pred_image_mode.astype(int)])\n",
    "    plt.show()\n",
    "    \n",
    "    # Save\n",
    "    scipy.io.savemat(SAVE_PATH[indx_model], {'seabedmap_graciosa':pred_image_mode})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
